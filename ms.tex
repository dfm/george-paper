% Copyright 2015-2017 Dan Foreman-Mackey and the co-authors listed below.

\documentclass[modern, letterpaper]{aastex61}

\pdfoutput=1

\include{vc}

\usepackage{microtype}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{multirow}
\bibliographystyle{aasjournal}

% Matrix fix:
% http://tex.stackexchange.com/questions/317824/letter-c-appearing-inside-pmatrix-environment-with-aastex
%\makeatletter
%\def\env@matrix{\hskip -\arraycolsep\let\@ifnextchar\new@ifnextchar\array{*{\c@MaxMatrixCols}c}}
%\makeatother

% Column spacing in matrix
% http://tex.stackexchange.com/questions/275725/adjusting-separation-between-matrix-entries
\setlength\arraycolsep{25pt}

% Projects:
\newcommand{\project}[1]{\textsf{#1}}

\newcommand{\foreign}[1]{\emph{#1}}
\newcommand{\etal}{\foreign{et\,al.}}
\newcommand{\etc}{\foreign{etc.}}
\newcommand{\ie}{\foreign{i.e.}}

\newcommand{\figureref}[1]{\ref{fig:#1}}
\newcommand{\Figure}[1]{Figure~\figureref{#1}}
\newcommand{\figurelabel}[1]{\label{fig:#1}}

\newcommand{\Table}[1]{Table~\ref{tab:#1}}
\newcommand{\tablelabel}[1]{\label{tab:#1}}

\renewcommand{\eqref}[1]{\ref{eq:#1}}
\newcommand{\Eq}[1]{Equation~(\eqref{#1})}
\newcommand{\eq}[1]{\Eq{#1}}
\newcommand{\eqalt}[1]{Equation~\eqref{#1}}
\newcommand{\eqlabel}[1]{\label{eq:#1}}

\newcommand{\sectionname}{Section}
\newcommand{\sectref}[1]{\ref{sect:#1}}
\newcommand{\Sect}[1]{\sectionname~\sectref{#1}}
\newcommand{\sect}[1]{\Sect{#1}}
\newcommand{\sectalt}[1]{\sectref{#1}}
\newcommand{\App}[1]{Appendix~\sectref{#1}}
\newcommand{\app}[1]{\App{#1}}
\newcommand{\sectlabel}[1]{\label{sect:#1}}

\newcommand{\T}{\ensuremath{\mathrm{T}}}
\newcommand{\dd}{\ensuremath{\,\mathrm{d}}}
\newcommand{\unit}[1]{{\ensuremath{\,\mathrm{#1}}}}
\newcommand{\bvec}[1]{{\ensuremath{\boldsymbol{#1}}}}

% TO DOS
\newcommand{\todo}[3]{{\color{#2}\emph{#1}: #3}}
\newcommand{\dfmtodo}[1]{\todo{DFM}{red}{#1}}

\newcommand{\response}[1]{{\color{blue}#1}}

\begin{document}
\sloppy\sloppypar\raggedbottom\frenchspacing

\title{Gaussian processes in astronomy}

\correspondingauthor{Daniel Foreman-Mackey}
\email{formeman.mackey@gmail.com}

\author[0000-0002-9328-5652]{Daniel Foreman-Mackey}
\affiliation{NASA Sagan Fellow}
\affiliation{Astronomy Department, University of Washington, Seattle, WA}
\affiliation{Center for Computational Astrophysics, Flatiron Institute, New York, NY}

\begin{abstract}\noindent

    This is an abstract.

\end{abstract}

\keywords{%
 %methods: data analysis
 %---
 %methods: statistical
 %---
 %asteroseismology
 %---
 %stars: rotation
 %---
 %planetary systems
}

\section{Introduction}

Face.

\section{Notation}

\section{The Gaussian likelihood}

It is commonly assumed that a set of data points with error bars represent
independent measurements with Gaussian uncertainties of known variance.
Under this assumption, for a model $f(\bvec{x};\,\bvec{\theta})$, the
likelihood for a single data point $y_n$ measured at coordinates $\bvec{x}_n$
with error bar $\sigma_n$ is
\begin{eqnarray}
p(y_n\,|\,\bvec{x}_n,\,\sigma_n,\,\bvec{\theta}) &=&
    \frac{1}{\sqrt{2\,\pi\,{\sigma_n}^2}}\,\exp\left(
    -\frac{1}{2}\,\frac{{[y_n-f(\bvec{x};\,\bvec{\theta})]}^2}{{\sigma_n}^2}
    \right) \quad.
\end{eqnarray}
Therefore, the joint likelihood for a set of $N$ data points ${\{\bvec{x}_n,\,
y_n,\, \sigma_n\}}_{n=1}^N$ is
\begin{eqnarray}\eqlabel{ind-like}
p(\{y_n\}\,|\,\{\bvec{x}_n,\,\sigma_n\},\,\bvec{\theta}) &=&
    \prod_{n=1}^N\frac{1}{\sqrt{2\,\pi\,{\sigma_n}^2}}\,\exp\left(
    -\frac{1}{2}\,\frac{{[y_n-f(\bvec{x};\,\bvec{\theta})]}^2}{{\sigma_n}^2}
    \right) \quad.
\end{eqnarray}
It is common practice to work with the natural logarithm of this quantity
instead of the likelihood directly.
Taking the logarithm of \eq{ind-like}, we find
\begin{eqnarray}\eqlabel{ind-loglike}
\log p(\{y_n\}\,|\,\{\bvec{x}_n,\,\sigma_n\},\,\bvec{\theta}) &=&
    -\frac{1}{2}\,\sum_{n=1}^N\left[
    \frac{{[y_n-f(\bvec{x};\,\bvec{\theta})]}^2}{{\sigma_n}^2}
    +\log {(2\,\pi\,{\sigma_n}^2)}
    \right]\quad.
\end{eqnarray}
It is worth qualitatively considering the roles of the two terms in
\eq{ind-loglike} because this discussion will come up repeatedly throughout
this paper.
The first term within the square brackets is what is commonly referred to as
``$\chi^2$'' in the astronomy literature and it quantifies the
``goodness-of-fit'' of the model.
The second term quantifies the specificity of the model and penalizes overly
general models.
For fixed uncertainties $\{\sigma_n\}$, this second term is a constant with
respect to the parameters $\bvec{\theta}$ and maximizing the log-likelihood in
\eq{ind-loglike} is equivalent to minimizing $\chi^2$.
In other words, calculating $\chi^2$ requires assuming that the uncertainties
are independent Gaussians with known variance.

This more general formulation of the Gaussian likelihood function will come in
handy for our derivation of GP modeling shortly, but let's start with a
concrete example where minimizing $\chi^2$ is not sufficient.
It is not uncommon for uncertainties on astronomical quantities to be
underestimated or unknown.
In this case, we must fit for a parametric representation of the uncertainties
simultaneously with the model $f(\bvec{x};\,\bvec{\theta})$.
To do this, we might include another parameter, we'll call it $s$, to quantify
the amount by which the uncertainties are underestimated.
In this case, the likelihood becomes
\begin{eqnarray}\eqlabel{jitter-loglike}
\log p(\{y_n\}\,|\,\{\bvec{x}_n,\,\sigma_n\},\,\bvec{\theta},\,s) &=&
    -\frac{1}{2}\,\sum_{n=1}^N\left[
    \frac{{[y_n-f(\bvec{x};\,\bvec{\theta})]}^2}{{\sigma_n}^2 + s^2}
    +\log {(2\,\pi\,[{\sigma_n}^2+s^2])}
    \right]
\end{eqnarray}
and we can now maximize this with respect to both $\bvec{\theta}$ and $s$.

\section{A Gaussian process as a model of correlated noise}

One way to motivate the definition of a GP is to think of it as a model of
correlated noise.
The derive this, we start by re-writing \eq{ind-loglike} as a matrix equation
\begin{eqnarray}\eqlabel{ind-loglike-matrix}
\log p(\{y_n\}\,|\,\{\bvec{x}_n,\,\sigma_n\},\,\bvec{\theta}) &=&
    -\frac{1}{2}\,{\bvec{r}_\bvec{\theta}}^\T\,K^{-1}\,{\bvec{r}_\bvec{\theta}}
    -\frac{1}{2}\,\log\det K
    -\frac{N}{2}\,\log(2\,\pi)
\end{eqnarray}
where $\bvec{r}_\bvec{\theta}$ is the residual vector
\begin{eqnarray}
{\bvec{r}_\bvec{\theta}}^\T = \left(\begin{array}{ccc}
    y_1 - f(\bvec{x}_1;\,\bvec{\theta}) & \cdots &
    y_N - f(\bvec{x}_N;\,\bvec{\theta})
\end{array}\right)
\end{eqnarray}
and $K$ is the ``covariance matrix'' that is, in this case, diagonal
\begin{eqnarray}\eqlabel{diag-cov}
K = \left(\begin{array}{ccc}
    {\sigma_1}^2 & 0 & 0 \\
    0 & \ddots & 0 \\
    0 & 0 & {\sigma_N}^2
\end{array}\right) \quad.
\end{eqnarray}
Noting that, for a diagonal matrix like this $K$, the inverse is
\begin{eqnarray}
K^{-1} = \left(\begin{array}{ccc}
    1/{\sigma_1}^2 & 0 & 0 \\
    0 & \ddots & 0 \\
    0 & 0 & 1/{\sigma_N}^2
\end{array}\right) \quad.
\end{eqnarray}
and the log-determinant is
\begin{eqnarray}
    \log\det K &=& \log \prod_{n=1}^N {\sigma_n}^2 \\
    &=& \sum_{n=1}^N \log{\sigma_n}^2 \quad,
\end{eqnarray}
we can see that \eq{ind-loglike} and \eq{ind-loglike-matrix} are equivalent.

You might remember that, in the previous section, we assumed that the data
points are independent.
This assumption is expressed by the fact that the covariance matrix in
\eq{diag-cov} is diagonal~--~all the off-diagonal elements are zero.
In order to take correlated noise into account, we introduce non-zero
off-diagonal elements in the covariance $K$.
The $n,\,m$-th entry in the $N \times N$ matrix $K$ quantifies the covariance
between data points $n$ and $m$.
If we have some method of \emph{estimating} this covariance \foreign{a
priori}~--~much like how we often assume that we can estimate the diagonal
elements of this matrix~--~we can fill in this matrix $K$ and evaluate
\eq{ind-loglike-matrix} with this new, dense $K$.
However, it is often hard to estimate these covariances reliably and we will,
instead, fit for them.
In practice, we probably don't want to add the $N\,(N-3)/2 \sim N^2$
parameters that would be needed to fit for each entry in this matrix directly.
Instead, we parameterize this covariance using a functional form where the
$n,\,m$-th element of the matrix is given by
\begin{eqnarray}\eqlabel{kernel}
K_{n,\,m} &=& {\sigma_n}^2\,\delta_{n,\,m} +
    k(\bvec{x}_n,\,\bvec{x}_m;\,\bvec{\alpha})
\end{eqnarray}
where $\delta_{n,\,m}$ is the Kronecker delta, and $k(\bvec{x}_n,\,\bvec{x}_m;
\,\bvec{\alpha})$ is a function~--~parameterized by $\bvec{\alpha}$~--~that
captures the covariance between the data points $\bvec{x}_n$ and $\bvec{x}_m$.
This function $k(\bvec{x}_n,\,\bvec{x}_m;\,\bvec{\alpha})$ goes by a few names
in the literature, the most common of which are ``covariance function'' or
``kernel function''.
Throughout this paper, we will refer to this function as the covariance
function.
To indicate that the covariance matrix is now parameterized by
$\bvec{\alpha}$, we will typeset it as $K_\bvec{\alpha}$ and the
log-likelihood function becomes
\begin{eqnarray}\eqlabel{gp-loglike}
\log p(\{y_n\}\,|\,\{\bvec{x}_n,\,\sigma_n\},\,\bvec{\theta},\,\bvec{\alpha})
    &=&
    -\frac{1}{2}\,{\bvec{r}_\bvec{\theta}}^\T\,{K_\bvec{\alpha}}^{-1}\,
        {\bvec{r}_\bvec{\theta}}
    -\frac{1}{2}\,\log\det K_\bvec{\alpha}
    -\frac{N}{2}\,\log(2\,\pi) \quad.
\end{eqnarray}

It is worth noting that the case of independent data points
(\eqalt{ind-loglike-matrix}) is a special (restrictive) case of
\eq{gp-loglike} where
\begin{eqnarray}
k(\bvec{x}_n,\,\bvec{x}_m;\,\bvec{\alpha}) &=& 0 \quad.
\end{eqnarray}
In the case of GPs, we relax this assumption and choose a more flexible
covariance function that approximates the real covariance structure in the
data generation process.
If we fit this GP model to a dataset where the data are actually independent
and the diagonal variances ${\sigma_n}^2$ are correctly estimated, the second
term in \eq{gp-loglike} will drive the covariance function to zero and
correctly capture the fact that the data are independent.

One of the touchiest subjects in GP modeling is the choice of covariance
function and we will return to a detailed discussion of this in \dfmtodo{SOME
SECTION}, but first, let's consider a concrete example.

\subsection{Code availability}

Alongside this paper, we have released a well-tested and documented open
source software package that implements the method and all of the examples
discussed in these pages.
This software is available on GitHub
\url{https://github.com/dfm/george}\footnote{This version of the paper was
generated with git commit \texttt{\githash} (\gitdate).} and Zenodo
\dfmtodo{ADD ZENODO}, and it is made available under the MIT license.

\acknowledgments
It is a pleasure to thank
for helpful discussions informing the ideas and code presented here.

This work was performed in part under contract with the Jet Propulsion
Laboratory (JPL) funded by NASA through the Sagan Fellowship Program executed
by the NASA Exoplanet Science Institute.

%This research made use of the NASA \project{Astrophysics Data System} and the
%NASA Exoplanet Archive.
%The Exoplanet Archive is operated by the California Institute of Technology,
%under contract with NASA under the Exoplanet Exploration Program.

%This paper includes data collected by the \kepler\ Mission. Funding for the
%\kepler\ Mission is provided by the NASA Science Mission directorate.
%We are grateful to the entire \kepler\ team, past and present.
%These data were obtained from the Mikulski Archive for Space Telescopes
%(MAST).
%STScI is operated by the Association of Universities for Research in
%Astronomy, Inc., under NASA contract NAS5-26555.
%Support for MAST is provided by the NASA Office of Space Science via grant
%NNX13AC07G and by other grants and contracts.

%This research made use of Astropy, a community-developed core Python package
%for Astronomy \citep{Astropy-Collaboration:2013}.

\facility{Kepler}
\software{%
     %\project{AstroPy} \citep{Astropy-Collaboration:2013},
     %\project{corner.py} \citep{Foreman-Mackey:2016},
     %\project{Eigen} \citep{Guennebaud:2010},
     %\project{emcee} \citep{Foreman-Mackey:2013},
     %\project{george} \citep{Ambikasaran:2016},
     %\project{Julia} \citep{Bezanzon:2012},
     %\project{LAPACK} \citep{Anderson:1999},
     %\project{matplotlib} \citep{Hunter:2007},
     %\project{numpy} \citep{Van-Der-Walt:2011},
     %\project{transit} \citep{Foreman-Mackey:2016a},
     %\project{scipy} \citep{Jones:2001}.
}

%\vspace{5ex}
%\appendix

\bibliography{george}

\end{document}
